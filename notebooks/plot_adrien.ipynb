{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-06-12T19:20:33.087978Z",
     "start_time": "2025-06-12T19:20:32.509986Z"
    }
   },
   "source": [
    "import json\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from typing import List, Dict, Any, Tuple, Optional\n",
    "\n",
    "# ... (Keep _aggregate_batch_metric_per_epoch and _align_epoch_metric as they are) ...\n",
    "\n",
    "# --- Plotting Function (Modified _plot_metric) ---\n",
    "def _plot_metric(\n",
    "        fig_num: int, title: str, ylabel: str,\n",
    "        epoch_ticks: np.ndarray,\n",
    "        num_epochs: int,\n",
    "        output_dir: str,\n",
    "        flag_epochs_map: Dict[int, List[str]],\n",
    "        train_metric: Optional[List[float]] = None,\n",
    "        val_metric: Optional[List[float]] = None,\n",
    "        other_metrics: Optional[Dict[str, List[float]]] = None,\n",
    "        filename: str = \"plot.png\",\n",
    "        log_scale_y: bool = False\n",
    "):\n",
    "    plt.figure(fig_num, figsize=(14, 8))\n",
    "    plt.clf()\n",
    "\n",
    "    legend_handles_main = []\n",
    "\n",
    "    if train_metric and len(train_metric) == num_epochs:\n",
    "        p, = plt.plot(epoch_ticks, train_metric, label='Training', marker='.', markersize=5, linewidth=1.5)\n",
    "        legend_handles_main.append(p)\n",
    "    if val_metric and len(val_metric) == num_epochs:\n",
    "        p, = plt.plot(epoch_ticks, val_metric, label='Validation', marker='.', markersize=5, linewidth=1.5)\n",
    "        legend_handles_main.append(p)\n",
    "\n",
    "    if other_metrics:\n",
    "        for name, metric_data in other_metrics.items():\n",
    "            if len(metric_data) == num_epochs:\n",
    "                valid_indices = [i for i, val in enumerate(metric_data) if not np.isnan(val)]\n",
    "                if valid_indices:\n",
    "                    current_epoch_ticks_to_plot = epoch_ticks[valid_indices]\n",
    "                    current_metric_data_to_plot = np.array(metric_data)[valid_indices]\n",
    "                    p, = plt.plot(current_epoch_ticks_to_plot, current_metric_data_to_plot, label=name,\n",
    "                                  marker='.' if len(valid_indices) < 50 else None, linestyle='--', linewidth=1.5)\n",
    "                    legend_handles_main.append(p)\n",
    "\n",
    "    flag_legend_handles_map = {}\n",
    "    flag_color_map = {\n",
    "        \"OVERFITTING\": \"red\",\n",
    "        \"VAL_LOSS_PLATEAU\": \"orange\",\n",
    "        \"LOW_GRADIENT_NORM\": \"purple\",         # Original flag from training\n",
    "        \"LOW_WEIGHT_UPDATE_NORM\": \"brown\",     # Original flag from training\n",
    "        \"ADAM_STALLED\": \"cyan\",\n",
    "        \"SGD_STALLED\": \"teal\",\n",
    "        \"HIGH_SHARPNESS\": \"magenta\",\n",
    "        # New colors for plot-time threshold flags\n",
    "        \"PLOT_LOW_GRAD_NORM\": \"darkorchid\",   # Plot-time evaluation\n",
    "        \"PLOT_LOW_WU_NORM\": \"sienna\",         # Plot-time evaluation\n",
    "        \"PLOT_VAL_LOSS_PLATEAU\": \"gold\",      # Plot-time evaluation\n",
    "        \"PLOT_OVERFITTING\": \"crimson\",        # Plot-time evaluation\n",
    "        \"DEFAULT_FLAG_COLOR\": \"gray\"\n",
    "    }\n",
    "\n",
    "    if flag_epochs_map:\n",
    "        for epoch_flagged, flag_types_at_epoch in flag_epochs_map.items():\n",
    "            if epoch_flagged in epoch_ticks: # Ensure the epoch is one that's being plotted\n",
    "                for flag_type in flag_types_at_epoch:\n",
    "                    color = flag_color_map.get(flag_type, flag_color_map[\"DEFAULT_FLAG_COLOR\"])\n",
    "                    if flag_type not in flag_legend_handles_map:\n",
    "                        dummy_line = plt.Line2D([0], [0], color=color, linestyle=':', linewidth=2, label=f\"{flag_type} Event\")\n",
    "                        flag_legend_handles_map[flag_type] = dummy_line\n",
    "                    plt.axvline(x=epoch_flagged, color=color, linestyle=':', linewidth=1.5, alpha=0.7)\n",
    "\n",
    "    combined_legend_handles = legend_handles_main + sorted(list(flag_legend_handles_map.values()), key=lambda x: x.get_label())\n",
    "\n",
    "    plt.title(title, fontsize=16)\n",
    "    plt.xlabel(\"Epoch\", fontsize=12)\n",
    "    plt.ylabel(ylabel, fontsize=12)\n",
    "    if log_scale_y:\n",
    "        plt.yscale('log')\n",
    "    plt.grid(True, which=\"both\", ls=\"-\", alpha=0.5)\n",
    "    if combined_legend_handles:\n",
    "        plt.legend(handles=combined_legend_handles, fontsize=10)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(output_dir, filename))\n",
    "    plt.close(fig_num)\n",
    "    print(f\"Saved plot: {filename}\")\n",
    "\n",
    "\n",
    "# --- Main plotting function (plot_metrics_with_flags modified) ---\n",
    "def plot_metrics_with_flags(\n",
    "        json_file_path: str,\n",
    "        output_dir: str = \"experiment_plots\",\n",
    "        # New parameters for plot-time threshold checks\n",
    "        plot_threshold_low_grad_norm: Optional[float] = None,\n",
    "        plot_threshold_low_weight_update: Optional[float] = None,\n",
    "        plot_threshold_val_loss_plateau_delta: Optional[float] = None,\n",
    "        plot_threshold_val_loss_plateau_window: int = 5, # Default window for plateau check\n",
    "        plot_threshold_overfitting_window: int = 5, # Default window for overfitting check\n",
    "        plot_threshold_overfitting_min_epochs: int = 3 # Min epochs for val loss up / train loss down\n",
    "):\n",
    "    if not os.path.exists(json_file_path):\n",
    "        print(f\"Error: JSON file not found at {json_file_path}\")\n",
    "        return\n",
    "\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    print(f\"Loading metrics from: {json_file_path}\")\n",
    "    print(f\"Saving plots to: {output_dir}\")\n",
    "\n",
    "    with open(json_file_path, 'r') as f:\n",
    "        raw_data = json.load(f)\n",
    "\n",
    "    # ... (Standardize data structure access - keep this part as is) ...\n",
    "    epochs_data_dict = {}\n",
    "    flags_history_list = []\n",
    "\n",
    "    if isinstance(raw_data, dict): # Likely all_metrics.json\n",
    "        print(f\"Info: Input file {json_file_path} is a dictionary. Attempting to parse comprehensive metrics.\")\n",
    "        epochs_data_dict['epochs_completed'] = raw_data.get('epochs', [])\n",
    "        epochs_data_dict['train_loss_history'] = raw_data.get('train_loss_history', [])\n",
    "        epochs_data_dict['train_acc_history'] = raw_data.get('train_acc_history', [])\n",
    "        epochs_data_dict['val_loss_history'] = raw_data.get('val_loss_history', [])\n",
    "        epochs_data_dict['val_acc_history'] = raw_data.get('val_acc_history', [])\n",
    "        epochs_data_dict['lr_history'] = raw_data.get('lr_history', [])\n",
    "        flags_history_list = raw_data.get('optimization_flags_history', [])\n",
    "    elif isinstance(raw_data, list): # Likely epoch_metric.json\n",
    "        # ... (keep this section for handling simpler list-based JSON) ...\n",
    "        print(f\"Info: Input file {json_file_path} is a list (likely epoch_metric.json).\")\n",
    "        epochs_data_dict['epochs_completed'] = [r.get(\"epoch\") for r in raw_data if \"epoch\" in r]\n",
    "        epochs_data_dict['train_loss_history'] = [r.get(\"train_loss\") for r in raw_data if \"train_loss\" in r]\n",
    "        epochs_data_dict['train_acc_history'] = [r.get(\"train_acc\") for r in raw_data if \"train_acc\" in r]\n",
    "        epochs_data_dict['val_loss_history'] = [r.get(\"val_loss\") for r in raw_data if \"val_loss\" in r]\n",
    "        epochs_data_dict['val_acc_history'] = [r.get(\"val_acc\") for r in raw_data if \"val_acc\" in r]\n",
    "        epochs_data_dict['lr_history'] = [r.get(\"learning_rate\") for r in raw_data if \"learning_rate\" in r]\n",
    "        for key, val_list in epochs_data_dict.items():\n",
    "            epochs_data_dict[key] = [v for v in val_list if v is not None]\n",
    "    else:\n",
    "        print(f\"Error: Unknown JSON structure in {json_file_path}\")\n",
    "        return\n",
    "\n",
    "    epochs = epochs_data_dict.get('epochs_completed', [])\n",
    "    if not epochs:\n",
    "        print(\"No epoch data found ('epochs' key in JSON is missing or empty). Cannot generate plots.\")\n",
    "        return\n",
    "\n",
    "    num_epochs = len(epochs)\n",
    "    epoch_ticks = np.array(epochs)\n",
    "\n",
    "    train_loss = epochs_data_dict.get('train_loss_history', [])\n",
    "    val_loss = epochs_data_dict.get('val_loss_history', [])\n",
    "    train_acc = [a * 100 if isinstance(a, (int, float)) else float('nan') for a in epochs_data_dict.get('train_acc_history', [])]\n",
    "    val_acc = [a * 100 if isinstance(a, (int, float)) else float('nan') for a in epochs_data_dict.get('val_acc_history', [])]\n",
    "    lr_history = epochs_data_dict.get('lr_history', [])\n",
    "\n",
    "    # Extract original flags from JSON\n",
    "    flag_epochs_map: Dict[int, List[str]] = {}\n",
    "    if flags_history_list:\n",
    "        for flag_event in flags_history_list:\n",
    "            epoch = flag_event.get('epoch')\n",
    "            flag_type = flag_event.get('flag_type', 'UNKNOWN_FLAG')\n",
    "            if epoch is not None:\n",
    "                if epoch not in flag_epochs_map:\n",
    "                    flag_epochs_map[epoch] = []\n",
    "                if flag_type not in flag_epochs_map[epoch]: # Avoid duplicates of same original flag\n",
    "                    flag_epochs_map[epoch].append(flag_type)\n",
    "    elif isinstance(raw_data, dict):\n",
    "        print(\"No 'optimization_flags_history' found in JSON. Original flags will not be highlighted.\")\n",
    "\n",
    "    # --- Calculate metrics needed for plot-time threshold evaluation ---\n",
    "    epoch_avg_grad_norms = []\n",
    "    batch_grad_norms_history = raw_data.get('gradient_norms', []) if isinstance(raw_data, dict) else []\n",
    "    if batch_grad_norms_history:\n",
    "        epoch_avg_grad_norms = _aggregate_batch_metric_per_epoch(batch_grad_norms_history, epochs, 'total_norm')\n",
    "\n",
    "    aligned_wu_norms = []\n",
    "    weight_update_history = raw_data.get('weight_update_norm_history', []) if isinstance(raw_data, dict) else []\n",
    "    if weight_update_history:\n",
    "        aligned_wu_norms = _align_epoch_metric(weight_update_history, epochs, 'norm')\n",
    "\n",
    "    # --- Evaluate plot-time thresholds and add to flag_epochs_map ---\n",
    "\n",
    "    # Plot-time: LOW_GRADIENT_NORM\n",
    "    if plot_threshold_low_grad_norm is not None and epoch_avg_grad_norms and len(epoch_avg_grad_norms) == num_epochs:\n",
    "        for i, epoch_val in enumerate(epochs):\n",
    "            if not np.isnan(epoch_avg_grad_norms[i]) and epoch_avg_grad_norms[i] < plot_threshold_low_grad_norm:\n",
    "                flag_type = \"PLOT_LOW_GRAD_NORM\"\n",
    "                if epoch_val not in flag_epochs_map: flag_epochs_map[epoch_val] = []\n",
    "                if flag_type not in flag_epochs_map[epoch_val]: flag_epochs_map[epoch_val].append(flag_type)\n",
    "\n",
    "    # Plot-time: LOW_WEIGHT_UPDATE_NORM\n",
    "    if plot_threshold_low_weight_update is not None and aligned_wu_norms and len(aligned_wu_norms) == num_epochs:\n",
    "        for i, epoch_val in enumerate(epochs):\n",
    "            if not np.isnan(aligned_wu_norms[i]) and aligned_wu_norms[i] < plot_threshold_low_weight_update:\n",
    "                flag_type = \"PLOT_LOW_WU_NORM\"\n",
    "                if epoch_val not in flag_epochs_map: flag_epochs_map[epoch_val] = []\n",
    "                if flag_type not in flag_epochs_map[epoch_val]: flag_epochs_map[epoch_val].append(flag_type)\n",
    "\n",
    "    # Plot-time: VAL_LOSS_PLATEAU\n",
    "    if plot_threshold_val_loss_plateau_delta is not None and val_loss and len(val_loss) == num_epochs:\n",
    "        if num_epochs >= plot_threshold_val_loss_plateau_window:\n",
    "            for i in range(plot_threshold_val_loss_plateau_window - 1, num_epochs):\n",
    "                window_val_losses = val_loss[i - plot_threshold_val_loss_plateau_window + 1 : i + 1]\n",
    "                if all(not np.isnan(x) for x in window_val_losses): # Ensure no NaNs in window\n",
    "                    net_improvement = window_val_losses[0] - window_val_losses[-1]\n",
    "                    if net_improvement < plot_threshold_val_loss_plateau_delta:\n",
    "                        epoch_val = epochs[i]\n",
    "                        flag_type = \"PLOT_VAL_LOSS_PLATEAU\"\n",
    "                        if epoch_val not in flag_epochs_map: flag_epochs_map[epoch_val] = []\n",
    "                        if flag_type not in flag_epochs_map[epoch_val]: flag_epochs_map[epoch_val].append(flag_type)\n",
    "\n",
    "    # Plot-time: OVERFITTING\n",
    "    if train_loss and val_loss and len(train_loss) == num_epochs and len(val_loss) == num_epochs:\n",
    "        if num_epochs >= plot_threshold_overfitting_window:\n",
    "            for i in range(plot_threshold_overfitting_window - 1, num_epochs):\n",
    "                window_train_losses = train_loss[i - plot_threshold_overfitting_window + 1 : i + 1]\n",
    "                window_val_losses = val_loss[i - plot_threshold_overfitting_window + 1 : i + 1]\n",
    "\n",
    "                if all(not np.isnan(x) for x in window_train_losses) and \\\n",
    "                        all(not np.isnan(x) for x in window_val_losses):\n",
    "                    val_loss_increasing_count = sum(1 for k in range(1, plot_threshold_overfitting_window) if window_val_losses[k] > window_val_losses[k-1])\n",
    "                    train_loss_decreasing_count = sum(1 for k in range(1, plot_threshold_overfitting_window) if window_train_losses[k] < window_train_losses[k-1])\n",
    "\n",
    "                    if val_loss_increasing_count >= plot_threshold_overfitting_min_epochs and \\\n",
    "                            train_loss_decreasing_count >= plot_threshold_overfitting_min_epochs:\n",
    "                        epoch_val = epochs[i]\n",
    "                        flag_type = \"PLOT_OVERFITTING\"\n",
    "                        if epoch_val not in flag_epochs_map: flag_epochs_map[epoch_val] = []\n",
    "                        if flag_type not in flag_epochs_map[epoch_val]: flag_epochs_map[epoch_val].append(flag_type)\n",
    "\n",
    "\n",
    "    # --- Generate Plots (Calls to _plot_metric are the same, but flag_epochs_map is now augmented) ---\n",
    "    # ... (Length mismatch warnings - keep as is) ...\n",
    "    # ... (Calls to _plot_metric for loss, accuracy, LR, etc. - keep as is, ensuring\n",
    "    #      epoch_ticks, num_epochs, output_dir, flag_epochs_map are passed correctly)\n",
    "\n",
    "    _plot_metric(1, \"Training & Validation Loss vs. Epoch\", \"Loss\",\n",
    "                 epoch_ticks=epoch_ticks, num_epochs=num_epochs, output_dir=output_dir, flag_epochs_map=flag_epochs_map,\n",
    "                 train_metric=train_loss if train_loss and len(train_loss) == num_epochs else None,\n",
    "                 val_metric=val_loss if val_loss and len(val_loss) == num_epochs else None,\n",
    "                 filename=\"loss_vs_epoch.png\", log_scale_y=True)\n",
    "\n",
    "    _plot_metric(2, \"Training & Validation Accuracy vs. Epoch\", \"Accuracy (%)\",\n",
    "                 epoch_ticks=epoch_ticks, num_epochs=num_epochs, output_dir=output_dir, flag_epochs_map=flag_epochs_map,\n",
    "                 train_metric=train_acc if train_acc and len(train_acc) == num_epochs else None,\n",
    "                 val_metric=val_acc if val_acc and len(val_acc) == num_epochs else None,\n",
    "                 filename=\"accuracy_vs_epoch.png\")\n",
    "\n",
    "    _plot_metric(3, \"Learning Rate vs. Epoch\", \"Learning Rate\",\n",
    "                 epoch_ticks=epoch_ticks, num_epochs=num_epochs, output_dir=output_dir, flag_epochs_map=flag_epochs_map,\n",
    "                 other_metrics={\"Learning Rate\": lr_history if lr_history and len(lr_history) == num_epochs else [float('nan')] * num_epochs},\n",
    "                 filename=\"lr_vs_epoch.png\", log_scale_y=True)\n",
    "\n",
    "    if epoch_avg_grad_norms and any(not np.isnan(x) for x in epoch_avg_grad_norms):\n",
    "        _plot_metric(4, \"Average Gradient Norm vs. Epoch\", \"Avg L2 Norm of Gradients\",\n",
    "                     epoch_ticks=epoch_ticks, num_epochs=num_epochs, output_dir=output_dir, flag_epochs_map=flag_epochs_map,\n",
    "                     other_metrics={\"Avg Grad Norm\": epoch_avg_grad_norms},\n",
    "                     filename=\"grad_norm_vs_epoch.png\", log_scale_y=True)\n",
    "\n",
    "    if aligned_wu_norms and any(not np.isnan(x) for x in aligned_wu_norms):\n",
    "        _plot_metric(5, \"Weight Update Norm vs. Epoch\", \"L2 Norm of Weight Updates\",\n",
    "                     epoch_ticks=epoch_ticks, num_epochs=num_epochs, output_dir=output_dir, flag_epochs_map=flag_epochs_map,\n",
    "                     other_metrics={\"Weight Update Norm\": aligned_wu_norms},\n",
    "                     filename=\"weight_update_norm_vs_epoch.png\", log_scale_y=True)\n",
    "\n",
    "    # ... (Keep plotting for optimizer state, noise, etc. as is, passing the new flag_epochs_map) ...\n",
    "    optimizer_state_history = raw_data.get('optimizer_state_history', []) if isinstance(raw_data, dict) else []\n",
    "    if optimizer_state_history:\n",
    "        aligned_m_norms = _align_epoch_metric(optimizer_state_history, epochs, 'avg_m_norm')\n",
    "        aligned_eff_lrs = _align_epoch_metric(optimizer_state_history, epochs, 'avg_eff_lr')\n",
    "        aligned_momentum_norms = _align_epoch_metric(optimizer_state_history, epochs, 'avg_momentum_norm')\n",
    "\n",
    "        adam_metrics_to_plot = {}\n",
    "        if any(not np.isnan(x) for x in aligned_m_norms): adam_metrics_to_plot['Avg Adam m_norm'] = aligned_m_norms\n",
    "        if any(not np.isnan(x) for x in aligned_eff_lrs): adam_metrics_to_plot['Avg Adam eff_LR'] = aligned_eff_lrs\n",
    "\n",
    "        sgd_metrics_to_plot = {}\n",
    "        if any(not np.isnan(x) for x in aligned_momentum_norms): sgd_metrics_to_plot['Avg SGD Momentum Norm'] = aligned_momentum_norms\n",
    "\n",
    "        if adam_metrics_to_plot:\n",
    "            _plot_metric(6, \"Adam Optimizer State vs. Epoch\", \"Metric Value\",\n",
    "                         epoch_ticks=epoch_ticks, num_epochs=num_epochs, output_dir=output_dir, flag_epochs_map=flag_epochs_map,\n",
    "                         other_metrics=adam_metrics_to_plot, filename=\"adam_state_vs_epoch.png\", log_scale_y=True)\n",
    "        if sgd_metrics_to_plot:\n",
    "            _plot_metric(7, \"SGD Optimizer State vs. Epoch\", \"Avg Momentum Norm\",\n",
    "                         epoch_ticks=epoch_ticks, num_epochs=num_epochs, output_dir=output_dir, flag_epochs_map=flag_epochs_map,\n",
    "                         other_metrics=sgd_metrics_to_plot, filename=\"sgd_state_vs_epoch.png\", log_scale_y=True)\n",
    "\n",
    "\n",
    "    noise_history = raw_data.get('noise_metrics_history', []) if isinstance(raw_data, dict) else []\n",
    "    if noise_history:\n",
    "        aligned_noise_mags = _align_epoch_metric(noise_history, epochs, 'magnitude')\n",
    "        if any(not np.isnan(x) for x in aligned_noise_mags):\n",
    "            _plot_metric(8, \"Noise Magnitude vs. Epoch\", \"Magnitude\",\n",
    "                         epoch_ticks=epoch_ticks, num_epochs=num_epochs, output_dir=output_dir, flag_epochs_map=flag_epochs_map,\n",
    "                         other_metrics={\"Noise Magnitude\": aligned_noise_mags},\n",
    "                         filename=\"noise_magnitude_vs_epoch.png\")\n",
    "\n",
    "\n",
    "    print(f\"Finished generating plots in {output_dir}.\")\n",
    "\n",
    "\n",
    "# --- Example usage ---\n",
    "# json_file = r\"path_to_your/all_metrics.json\"\n",
    "# output_plot_dir = \"my_experiment_plots_with_thresholds\"\n",
    "\n",
    "# plot_metrics_with_flags(\n",
    "#     json_file_path=json_file,\n",
    "#     output_dir=output_plot_dir,\n",
    "#     plot_threshold_low_grad_norm=1e-5,       # Example: Highlight if avg grad norm < 1e-5\n",
    "#     plot_threshold_low_weight_update=1e-4, # Example: Highlight if weight update norm < 1e-4\n",
    "#     plot_threshold_val_loss_plateau_delta=0.0001, # Example plateau delta\n",
    "#     plot_threshold_val_loss_plateau_window=5,\n",
    "#     plot_threshold_overfitting_window=5,\n",
    "#     plot_threshold_overfitting_min_epochs=2\n",
    "# )"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-12T19:20:33.115197Z",
     "start_time": "2025-06-12T19:20:33.109213Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Replace with the actual path to your metrics file\n",
    "# Ensure the path uses forward slashes or is a raw string if using backslashes\n",
    "json_file = r\"C:\\Users\\Home\\Documents\\EPFL\\OptML\\OptML\\PyTorch_CIFAR10\\checkpoints\\metrics\\resnet18_none_20250515_094307/all_metrics.json\"\n",
    "output_plot_dir = \"my_experiment_plots\"\n",
    "\n",
    "plot_metrics_with_flags(json_file_path=json_file,\n",
    "     output_dir=output_plot_dir,\n",
    "    plot_threshold_low_grad_norm=1,       # Example: Highlight if avg grad norm < 1e-5\n",
    "     plot_threshold_low_weight_update=10, # Example: Highlight if weight update norm < 1e-4\n",
    "     plot_threshold_val_loss_plateau_delta=0.0001, # Example plateau delta\n",
    "    plot_threshold_val_loss_plateau_window=10,\n",
    "     plot_threshold_overfitting_window=10,\n",
    "     plot_threshold_overfitting_min_epochs=5\n",
    ")"
   ],
   "id": "d6b8135ce3a8dd00",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: JSON file not found at C:\\Users\\Home\\Documents\\EPFL\\OptML\\OptML\\PyTorch_CIFAR10\\checkpoints\\metrics\\resnet18_none_20250515_094307/all_metrics.json\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-12T19:20:33.244056Z",
     "start_time": "2025-06-12T19:20:33.238733Z"
    }
   },
   "cell_type": "code",
   "source": [
    "json_file = r\"C:\\Users\\Home\\Documents\\EPFL\\OptML\\OptML\\PyTorch_CIFAR10\\checkpoints\\metrics\\resnet18_none_20250602_143348\\all_metrics.json\"\n",
    "output_plot_dir = \"my_experiment_plots_last_try\"\n",
    "\n",
    "plot_metrics_with_flags(json_file_path=json_file,\n",
    "                        output_dir=output_plot_dir,\n",
    "                        plot_threshold_low_grad_norm=1e-3,       # Example: Highlight if avg grad norm < 1e-5\n",
    "                        plot_threshold_low_weight_update=1e-2, # Example: Highlight if weight update norm < 1e-4\n",
    "                        plot_threshold_val_loss_plateau_delta=0.0001, # Example plateau delta\n",
    "                        plot_threshold_val_loss_plateau_window=10,\n",
    "                        plot_threshold_overfitting_window=10,\n",
    "                        plot_threshold_overfitting_min_epochs=5\n",
    "                        \n",
    "                        \n",
    "                        )"
   ],
   "id": "91a6da690d34dc92",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: JSON file not found at C:\\Users\\Home\\Documents\\EPFL\\OptML\\OptML\\PyTorch_CIFAR10\\checkpoints\\metrics\\resnet18_none_20250602_143348\\all_metrics.json\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-12T19:20:33.294435Z",
     "start_time": "2025-06-12T19:20:33.279651Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import json\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from typing import List, Dict, Any, Tuple, Optional\n",
    "\n",
    "# --- Helper functions (These can remain as functions) ---\n",
    "\n",
    "def _aggregate_batch_metric_per_epoch(batch_metrics_history: List[Dict[str, Any]], epochs: List[int], metric_key: str) -> List[float]:\n",
    "    \"\"\"Aggregates a batch-level metric to an average per epoch.\"\"\"\n",
    "    epoch_to_sum = {epoch_num: 0.0 for epoch_num in epochs}\n",
    "    epoch_to_count = {epoch_num: 0 for epoch_num in epochs}\n",
    "\n",
    "    for entry in batch_metrics_history:\n",
    "        epoch_num = entry.get('epoch')\n",
    "        metric_val = entry.get(metric_key)\n",
    "        if epoch_num is not None and metric_val is not None:\n",
    "            epoch_to_sum[epoch_num] += metric_val\n",
    "            epoch_to_count[epoch_num] += 1\n",
    "\n",
    "    aggregated_metrics = []\n",
    "    for epoch_num in epochs:\n",
    "        if epoch_to_count[epoch_num] > 0:\n",
    "            aggregated_metrics.append(epoch_to_sum[epoch_num] / epoch_to_count[epoch_num])\n",
    "        else:\n",
    "            aggregated_metrics.append(float('nan')) # No data for this epoch\n",
    "    return aggregated_metrics\n",
    "\n",
    "def _align_epoch_metric(history: List[Dict[str, Any]], target_epochs: List[int], metric_key: str) -> List[float]:\n",
    "    \"\"\"Aligns a list of metric dictionaries to a target list of epochs.\"\"\"\n",
    "    aligned_values = [float('nan')] * len(target_epochs)\n",
    "    epoch_to_index = {epoch: i for i, epoch in enumerate(target_epochs)}\n",
    "\n",
    "    for entry in history:\n",
    "        epoch = entry.get('epoch')\n",
    "        value = entry.get(metric_key)\n",
    "        if epoch is not None and value is not None and epoch in epoch_to_index:\n",
    "            aligned_values[epoch_to_index[epoch]] = value\n",
    "    return aligned_values\n",
    "\n",
    "def _plot_metric(\n",
    "        fig_num: int, title: str, ylabel: str,\n",
    "        epoch_ticks: np.ndarray,\n",
    "        num_epochs: int,\n",
    "        output_dir: str,\n",
    "        flag_epochs_map: Dict[int, List[str]],\n",
    "        train_metric: Optional[List[float]] = None,\n",
    "        val_metric: Optional[List[float]] = None,\n",
    "        other_metrics: Optional[Dict[str, List[float]]] = None,\n",
    "        filename: str = \"plot.png\",\n",
    "        log_scale_y: bool = False\n",
    "):\n",
    "    plt.figure(fig_num, figsize=(14, 8))\n",
    "    plt.clf() # Clear the current figure to prevent plots from overlapping on successive runs\n",
    "\n",
    "    legend_handles_main = []\n",
    "\n",
    "    if train_metric is not None and len(train_metric) == num_epochs: # Check for None explicitly\n",
    "        p, = plt.plot(epoch_ticks, train_metric, label='Training', marker='.', markersize=5, linewidth=1.5)\n",
    "        legend_handles_main.append(p)\n",
    "    if val_metric is not None and len(val_metric) == num_epochs: # Check for None explicitly\n",
    "        p, = plt.plot(epoch_ticks, val_metric, label='Validation', marker='.', markersize=5, linewidth=1.5)\n",
    "        legend_handles_main.append(p)\n",
    "\n",
    "    if other_metrics:\n",
    "        for name, metric_data in other_metrics.items():\n",
    "            if metric_data is not None and len(metric_data) == num_epochs: # Check for None explicitly\n",
    "                valid_indices = [i for i, val in enumerate(metric_data) if not np.isnan(val)]\n",
    "                if valid_indices:\n",
    "                    current_epoch_ticks_to_plot = epoch_ticks[valid_indices]\n",
    "                    current_metric_data_to_plot = np.array(metric_data)[valid_indices]\n",
    "                    p, = plt.plot(current_epoch_ticks_to_plot, current_metric_data_to_plot, label=name,\n",
    "                                  marker='.' if len(valid_indices) < 50 else None, linestyle='--', linewidth=1.5)\n",
    "                    legend_handles_main.append(p)\n",
    "\n",
    "    flag_legend_handles_map = {}\n",
    "    flag_color_map = {\n",
    "        \"OVERFITTING\": \"red\",\n",
    "        \"VAL_LOSS_PLATEAU\": \"orange\",\n",
    "        \"LOW_GRADIENT_NORM\": \"purple\",\n",
    "        \"LOW_WEIGHT_UPDATE_NORM\": \"brown\",\n",
    "        \"ADAM_STALLED\": \"cyan\",\n",
    "        \"SGD_STALLED\": \"teal\",\n",
    "        \"HIGH_SHARPNESS\": \"magenta\",\n",
    "        \"DEFAULT_FLAG_COLOR\": \"gray\"\n",
    "    }\n",
    "\n",
    "    if flag_epochs_map:\n",
    "        for epoch_flagged, flag_types_at_epoch in flag_epochs_map.items():\n",
    "            if epoch_flagged in epoch_ticks:\n",
    "                for flag_type in flag_types_at_epoch:\n",
    "                    color = flag_color_map.get(flag_type, flag_color_map[\"DEFAULT_FLAG_COLOR\"])\n",
    "                    if flag_type not in flag_legend_handles_map:\n",
    "                        dummy_line = plt.Line2D([0], [0], color=color, linestyle=':', linewidth=2, label=f\"{flag_type} Event\")\n",
    "                        flag_legend_handles_map[flag_type] = dummy_line\n",
    "                    plt.axvline(x=epoch_flagged, color=color, linestyle=':', linewidth=1.5, alpha=0.7)\n",
    "\n",
    "    combined_legend_handles = legend_handles_main + sorted(list(flag_legend_handles_map.values()), key=lambda x: x.get_label())\n",
    "\n",
    "    plt.title(title, fontsize=16)\n",
    "    plt.xlabel(\"Epoch\", fontsize=12)\n",
    "    plt.ylabel(ylabel, fontsize=12)\n",
    "    if log_scale_y:\n",
    "        plt.yscale('log')\n",
    "    plt.grid(True, which=\"both\", ls=\"-\", alpha=0.5)\n",
    "    if combined_legend_handles:\n",
    "        plt.legend(handles=combined_legend_handles, fontsize=10)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(output_dir, filename))\n",
    "    plt.show() # Display plot in Jupyter notebook\n",
    "    plt.close(fig_num) # Close figure to free memory after showing\n",
    "    print(f\"Saved plot: {filename}\")\n"
   ],
   "id": "47530c9a7db7d918",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-12T19:20:33.396422Z",
     "start_time": "2025-06-12T19:20:33.382033Z"
    }
   },
   "cell_type": "code",
   "source": [
    "json_file = r\"C:\\Users\\Home\\Documents\\EPFL\\OptML\\OptML\\PyTorch_CIFAR10\\checkpoints\\metrics\\baby_cnn_none_20250609_190202\\all_metrics.json\" # <<< UPDATE THIS PATH\n",
    "output_plot_dir = \"my_experiment_plots\" # Folder to save plots\n",
    "\n",
    "# Create output directory if it doesn't exist\n",
    "os.makedirs(output_plot_dir, exist_ok=True)\n",
    "print(f\"Loading metrics from: {json_file}\")\n",
    "print(f\"Saving plots to: {output_plot_dir}\")\n",
    "\n",
    "# 2. Load and parse data\n",
    "if not os.path.exists(json_file):\n",
    "    print(f\"Error: JSON file not found at {json_file}\")\n",
    "else:\n",
    "    with open(json_file, 'r') as f:\n",
    "        raw_data = json.load(f)\n",
    "\n",
    "    epochs_data_dict = {}\n",
    "    flags_history_list = []\n",
    "\n",
    "    if isinstance(raw_data, dict):\n",
    "        print(f\"Info: Input file {json_file} is a dictionary. Parsing comprehensive metrics.\")\n",
    "        epochs_data_dict['epochs_completed'] = raw_data.get('epochs', [])\n",
    "        epochs_data_dict['train_loss_history'] = raw_data.get('train_loss_history', [])\n",
    "        epochs_data_dict['train_acc_history'] = raw_data.get('train_acc_history', [])\n",
    "        epochs_data_dict['val_loss_history'] = raw_data.get('val_loss_history', [])\n",
    "        epochs_data_dict['val_acc_history'] = raw_data.get('val_acc_history', [])\n",
    "        epochs_data_dict['lr_history'] = raw_data.get('lr_history', [])\n",
    "        flags_history_list = raw_data.get('optimization_flags_history', [])\n",
    "        # Correctly extract noise metrics\n",
    "        noise_metrics_history = raw_data.get('noise_metrics', [])\n",
    "\n",
    "    elif isinstance(raw_data, list): # Fallback for older JSON structure if needed\n",
    "        print(f\"Info: Input file {json_file} is a list. This format might not contain all desired metrics.\")\n",
    "        epochs_data_dict['epochs_completed'] = [r.get(\"epoch\") for r in raw_data if \"epoch\" in r]\n",
    "        epochs_data_dict['train_loss_history'] = [r.get(\"train_loss\") for r in raw_data if \"train_loss\" in r]\n",
    "        epochs_data_dict['train_acc_history'] = [r.get(\"train_acc\") for r in raw_data if \"train_acc\" in r]\n",
    "        epochs_data_dict['val_loss_history'] = [r.get(\"val_loss\") for r in raw_data if \"val_loss\" in r]\n",
    "        epochs_data_dict['val_acc_history'] = [r.get(\"val_acc\") for r in raw_data if \"val_acc\" in r]\n",
    "        epochs_data_dict['lr_history'] = [r.get(\"learning_rate\") for r in raw_data if \"learning_rate\" in r]\n",
    "        noise_metrics_history = [] # No noise metrics in this older format, or handle specifically\n",
    "        for key, val_list in epochs_data_dict.items():\n",
    "            epochs_data_dict[key] = [v for v in val_list if v is not None]\n",
    "    else:\n",
    "        print(f\"Error: Unknown JSON structure in {json_file}\")\n",
    "        # Exit or handle error appropriately if not running in a function\n",
    "        exit()\n",
    "\n",
    "    epochs = epochs_data_dict.get('epochs_completed', [])\n",
    "    if not epochs:\n",
    "        print(\"No epoch data found. Cannot generate plots.\")\n",
    "        exit() # Exit the script if no data\n",
    "\n",
    "    num_epochs = len(epochs)\n",
    "    epoch_ticks = np.array(epochs)\n",
    "\n",
    "    train_loss = epochs_data_dict.get('train_loss_history', [])\n",
    "    val_loss = epochs_data_dict.get('val_loss_history', [])\n",
    "    train_acc = [a * 100 if isinstance(a, (int, float)) else float('nan') for a in epochs_data_dict.get('train_acc_history', [])]\n",
    "    val_acc = [a * 100 if isinstance(a, (int, float)) else float('nan') for a in epochs_data_dict.get('val_acc_history', [])]\n",
    "    lr_history = epochs_data_dict.get('lr_history', [])\n",
    "\n",
    "\n",
    "    # Extract actual triggered flags\n",
    "    flag_epochs_map: Dict[int, List[str]] = {}\n",
    "    if flags_history_list:\n",
    "        for flag_event in flags_history_list:\n",
    "            epoch = flag_event.get('epoch')\n",
    "            flag_type = flag_event.get('flag_type', 'UNKNOWN_FLAG')\n",
    "            if epoch is not None:\n",
    "                if epoch not in flag_epochs_map:\n",
    "                    flag_epochs_map[epoch] = []\n",
    "                if flag_type not in flag_epochs_map[epoch]:\n",
    "                    flag_epochs_map[epoch].append(flag_type)\n",
    "    else:\n",
    "        print(\"No 'optimization_flags_history' found in JSON. Triggered flags from training will not be highlighted.\")\n",
    "\n",
    "    # Calculate metrics for plotting (even if not used for triggering)\n",
    "    epoch_avg_grad_norms = _aggregate_batch_metric_per_epoch(raw_data.get('gradient_norms', []), epochs, 'total_norm')\n",
    "    aligned_wu_norms = _align_epoch_metric(raw_data.get('weight_update_norm_history', []), epochs, 'norm')\n",
    "    aligned_noise_mags = _align_epoch_metric(noise_metrics_history, epochs, 'magnitude') # Corrected source for noise magnitudes\n",
    "\n",
    "    # 3. Generate Plots\n",
    "    _plot_metric(1, \"Training & Validation Loss vs. Epoch\", \"Loss\",\n",
    "                 epoch_ticks=epoch_ticks, num_epochs=num_epochs, output_dir=output_plot_dir, flag_epochs_map=flag_epochs_map,\n",
    "                 train_metric=train_loss, val_metric=val_loss,\n",
    "                 filename=\"loss_vs_epoch.png\", log_scale_y=True)\n",
    "\n",
    "    _plot_metric(2, \"Training & Validation Accuracy vs. Epoch\", \"Accuracy (%)\",\n",
    "                 epoch_ticks=epoch_ticks, num_epochs=num_epochs, output_dir=output_plot_dir, flag_epochs_map=flag_epochs_map,\n",
    "                 train_metric=train_acc, val_metric=val_acc,\n",
    "                 filename=\"accuracy_vs_epoch.png\")\n",
    "\n",
    "    _plot_metric(3, \"Learning Rate vs. Epoch\", \"Learning Rate\",\n",
    "                 epoch_ticks=epoch_ticks, num_epochs=num_epochs, output_dir=output_plot_dir, flag_epochs_map=flag_epochs_map,\n",
    "                 other_metrics={\"Learning Rate\": lr_history},\n",
    "                 filename=\"lr_vs_epoch.png\", log_scale_y=True)\n",
    "\n",
    "    if epoch_avg_grad_norms and any(not np.isnan(x) for x in epoch_avg_grad_norms):\n",
    "        _plot_metric(4, \"Average Gradient Norm vs. Epoch\", \"Avg L2 Norm of Gradients\",\n",
    "                     epoch_ticks=epoch_ticks, num_epochs=num_epochs, output_dir=output_plot_dir, flag_epochs_map=flag_epochs_map,\n",
    "                     other_metrics={\"Avg Grad Norm\": epoch_avg_grad_norms},\n",
    "                     filename=\"grad_norm_vs_epoch.png\", log_scale_y=True)\n",
    "\n",
    "    if aligned_wu_norms and any(not np.isnan(x) for x in aligned_wu_norms):\n",
    "        _plot_metric(5, \"Weight Update Norm vs. Epoch\", \"L2 Norm of Weight Updates\",\n",
    "                     epoch_ticks=epoch_ticks, num_epochs=num_epochs, output_dir=output_plot_dir, flag_epochs_map=flag_epochs_map,\n",
    "                     other_metrics={\"Weight Update Norm\": aligned_wu_norms},\n",
    "                     filename=\"weight_update_norm_vs_epoch.png\", log_scale_y=True)\n",
    "\n",
    "    optimizer_state_history = raw_data.get('optimizer_state_history', [])\n",
    "    if optimizer_state_history:\n",
    "        aligned_m_norms = _align_epoch_metric(optimizer_state_history, epochs, 'avg_m_norm')\n",
    "        aligned_eff_lrs = _align_epoch_metric(optimizer_state_history, epochs, 'avg_eff_lr')\n",
    "        aligned_momentum_norms = _align_epoch_metric(optimizer_state_history, epochs, 'avg_momentum_norm')\n",
    "\n",
    "        adam_metrics_to_plot = {}\n",
    "        if aligned_m_norms and any(not np.isnan(x) for x in aligned_m_norms): adam_metrics_to_plot['Avg Adam m_norm'] = aligned_m_norms\n",
    "        if aligned_eff_lrs and any(not np.isnan(x) for x in aligned_eff_lrs): adam_metrics_to_plot['Avg Adam eff_LR'] = aligned_eff_lrs\n",
    "\n",
    "        sgd_metrics_to_plot = {}\n",
    "        if aligned_momentum_norms and any(not np.isnan(x) for x in aligned_momentum_norms): sgd_metrics_to_plot['Avg SGD Momentum Norm'] = aligned_momentum_norms\n",
    "\n",
    "        if adam_metrics_to_plot:\n",
    "            _plot_metric(6, \"Adam Optimizer State vs. Epoch\", \"Metric Value\",\n",
    "                         epoch_ticks=epoch_ticks, num_epochs=num_epochs, output_dir=output_plot_dir, flag_epochs_map=flag_epochs_map,\n",
    "                         other_metrics=adam_metrics_to_plot, filename=\"adam_state_vs_epoch.png\", log_scale_y=True)\n",
    "        if sgd_metrics_to_plot:\n",
    "            _plot_metric(7, \"SGD Optimizer State vs. Epoch\", \"Avg Momentum Norm\",\n",
    "                         epoch_ticks=epoch_ticks, num_epochs=num_epochs, output_dir=output_plot_dir, flag_epochs_map=flag_epochs_map,\n",
    "                         other_metrics=sgd_metrics_to_plot, filename=\"sgd_state_vs_epoch.png\", log_scale_y=True)\n",
    "\n",
    "    if aligned_noise_mags and any(not np.isnan(x) for x in aligned_noise_mags):\n",
    "        _plot_metric(8, \"Noise Magnitude vs. Epoch\", \"Magnitude\",\n",
    "                     epoch_ticks=epoch_ticks, num_epochs=num_epochs, output_dir=output_plot_dir, flag_epochs_map=flag_epochs_map,\n",
    "                     other_metrics={\"Noise Magnitude\": aligned_noise_mags},\n",
    "                     filename=\"noise_magnitude_vs_epoch.png\")\n",
    "\n",
    "\n",
    "    print(f\"Finished generating plots in {output_plot_dir}.\")\n"
   ],
   "id": "d24616b870b23cd6",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading metrics from: C:\\Users\\Home\\Documents\\EPFL\\OptML\\OptML\\PyTorch_CIFAR10\\checkpoints\\metrics\\baby_cnn_none_20250609_190202\\all_metrics.json\n",
      "Saving plots to: my_experiment_plots\n",
      "Error: JSON file not found at C:\\Users\\Home\\Documents\\EPFL\\OptML\\OptML\\PyTorch_CIFAR10\\checkpoints\\metrics\\baby_cnn_none_20250609_190202\\all_metrics.json\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-12T19:20:33.457571Z",
     "start_time": "2025-06-12T19:20:33.442706Z"
    }
   },
   "cell_type": "code",
   "source": [
    "json_file = r\"C:\\Users\\Home\\Documents\\EPFL\\OptML\\OptML\\PyTorch_CIFAR10\\checkpoints\\metrics\\baby_cnn_gradient_gaussian_0.01_20250609_230947\\all_metrics.json\" # <<< UPDATE THIS PATH\n",
    "output_plot_dir = \"my_experiment_plots\" # Folder to save plots\n",
    "\n",
    "# Create output directory if it doesn't exist\n",
    "os.makedirs(output_plot_dir, exist_ok=True)\n",
    "print(f\"Loading metrics from: {json_file}\")\n",
    "print(f\"Saving plots to: {output_plot_dir}\")\n",
    "\n",
    "# 2. Load and parse data\n",
    "if not os.path.exists(json_file):\n",
    "    print(f\"Error: JSON file not found at {json_file}\")\n",
    "else:\n",
    "    with open(json_file, 'r') as f:\n",
    "        raw_data = json.load(f)\n",
    "\n",
    "    epochs_data_dict = {}\n",
    "    flags_history_list = []\n",
    "\n",
    "    if isinstance(raw_data, dict):\n",
    "        print(f\"Info: Input file {json_file} is a dictionary. Parsing comprehensive metrics.\")\n",
    "        epochs_data_dict['epochs_completed'] = raw_data.get('epochs', [])\n",
    "        epochs_data_dict['train_loss_history'] = raw_data.get('train_loss_history', [])\n",
    "        epochs_data_dict['train_acc_history'] = raw_data.get('train_acc_history', [])\n",
    "        epochs_data_dict['val_loss_history'] = raw_data.get('val_loss_history', [])\n",
    "        epochs_data_dict['val_acc_history'] = raw_data.get('val_acc_history', [])\n",
    "        epochs_data_dict['lr_history'] = raw_data.get('lr_history', [])\n",
    "        flags_history_list = raw_data.get('optimization_flags_history', [])\n",
    "        # Correctly extract noise metrics\n",
    "        noise_metrics_history = raw_data.get('noise_metrics', [])\n",
    "\n",
    "    elif isinstance(raw_data, list): # Fallback for older JSON structure if needed\n",
    "        print(f\"Info: Input file {json_file} is a list. This format might not contain all desired metrics.\")\n",
    "        epochs_data_dict['epochs_completed'] = [r.get(\"epoch\") for r in raw_data if \"epoch\" in r]\n",
    "        epochs_data_dict['train_loss_history'] = [r.get(\"train_loss\") for r in raw_data if \"train_loss\" in r]\n",
    "        epochs_data_dict['train_acc_history'] = [r.get(\"train_acc\") for r in raw_data if \"train_acc\" in r]\n",
    "        epochs_data_dict['val_loss_history'] = [r.get(\"val_loss\") for r in raw_data if \"val_loss\" in r]\n",
    "        epochs_data_dict['val_acc_history'] = [r.get(\"val_acc\") for r in raw_data if \"val_acc\" in r]\n",
    "        epochs_data_dict['lr_history'] = [r.get(\"learning_rate\") for r in raw_data if \"learning_rate\" in r]\n",
    "        noise_metrics_history = [] # No noise metrics in this older format, or handle specifically\n",
    "        for key, val_list in epochs_data_dict.items():\n",
    "            epochs_data_dict[key] = [v for v in val_list if v is not None]\n",
    "    else:\n",
    "        print(f\"Error: Unknown JSON structure in {json_file}\")\n",
    "        # Exit or handle error appropriately if not running in a function\n",
    "        exit()\n",
    "\n",
    "    epochs = epochs_data_dict.get('epochs_completed', [])\n",
    "    if not epochs:\n",
    "        print(\"No epoch data found. Cannot generate plots.\")\n",
    "        exit() # Exit the script if no data\n",
    "\n",
    "    num_epochs = len(epochs)\n",
    "    epoch_ticks = np.array(epochs)\n",
    "\n",
    "    train_loss = epochs_data_dict.get('train_loss_history', [])\n",
    "    val_loss = epochs_data_dict.get('val_loss_history', [])\n",
    "    train_acc = [a * 100 if isinstance(a, (int, float)) else float('nan') for a in epochs_data_dict.get('train_acc_history', [])]\n",
    "    val_acc = [a * 100 if isinstance(a, (int, float)) else float('nan') for a in epochs_data_dict.get('val_acc_history', [])]\n",
    "    lr_history = epochs_data_dict.get('lr_history', [])\n",
    "\n",
    "\n",
    "    # Extract actual triggered flags\n",
    "    flag_epochs_map: Dict[int, List[str]] = {}\n",
    "    if flags_history_list:\n",
    "        for flag_event in flags_history_list:\n",
    "            epoch = flag_event.get('epoch')\n",
    "            flag_type = flag_event.get('flag_type', 'UNKNOWN_FLAG')\n",
    "            if epoch is not None:\n",
    "                if epoch not in flag_epochs_map:\n",
    "                    flag_epochs_map[epoch] = []\n",
    "                if flag_type not in flag_epochs_map[epoch]:\n",
    "                    flag_epochs_map[epoch].append(flag_type)\n",
    "    else:\n",
    "        print(\"No 'optimization_flags_history' found in JSON. Triggered flags from training will not be highlighted.\")\n",
    "\n",
    "    # Calculate metrics for plotting (even if not used for triggering)\n",
    "    epoch_avg_grad_norms = _aggregate_batch_metric_per_epoch(raw_data.get('gradient_norms', []), epochs, 'total_norm')\n",
    "    aligned_wu_norms = _align_epoch_metric(raw_data.get('weight_update_norm_history', []), epochs, 'norm')\n",
    "    aligned_noise_mags = _align_epoch_metric(noise_metrics_history, epochs, 'magnitude') # Corrected source for noise magnitudes\n",
    "\n",
    "    # 3. Generate Plots\n",
    "    _plot_metric(1, \"Training & Validation Loss vs. Epoch\", \"Loss\",\n",
    "                 epoch_ticks=epoch_ticks, num_epochs=num_epochs, output_dir=output_plot_dir, flag_epochs_map=flag_epochs_map,\n",
    "                 train_metric=train_loss, val_metric=val_loss,\n",
    "                 filename=\"loss_vs_epoch.png\", log_scale_y=True)\n",
    "\n",
    "    _plot_metric(2, \"Training & Validation Accuracy vs. Epoch\", \"Accuracy (%)\",\n",
    "                 epoch_ticks=epoch_ticks, num_epochs=num_epochs, output_dir=output_plot_dir, flag_epochs_map=flag_epochs_map,\n",
    "                 train_metric=train_acc, val_metric=val_acc,\n",
    "                 filename=\"accuracy_vs_epoch.png\")\n",
    "\n",
    "    _plot_metric(3, \"Learning Rate vs. Epoch\", \"Learning Rate\",\n",
    "                 epoch_ticks=epoch_ticks, num_epochs=num_epochs, output_dir=output_plot_dir, flag_epochs_map=flag_epochs_map,\n",
    "                 other_metrics={\"Learning Rate\": lr_history},\n",
    "                 filename=\"lr_vs_epoch.png\", log_scale_y=True)\n",
    "\n",
    "    if epoch_avg_grad_norms and any(not np.isnan(x) for x in epoch_avg_grad_norms):\n",
    "        _plot_metric(4, \"Average Gradient Norm vs. Epoch\", \"Avg L2 Norm of Gradients\",\n",
    "                     epoch_ticks=epoch_ticks, num_epochs=num_epochs, output_dir=output_plot_dir, flag_epochs_map=flag_epochs_map,\n",
    "                     other_metrics={\"Avg Grad Norm\": epoch_avg_grad_norms},\n",
    "                     filename=\"grad_norm_vs_epoch.png\", log_scale_y=True)\n",
    "\n",
    "    if aligned_wu_norms and any(not np.isnan(x) for x in aligned_wu_norms):\n",
    "        _plot_metric(5, \"Weight Update Norm vs. Epoch\", \"L2 Norm of Weight Updates\",\n",
    "                     epoch_ticks=epoch_ticks, num_epochs=num_epochs, output_dir=output_plot_dir, flag_epochs_map=flag_epochs_map,\n",
    "                     other_metrics={\"Weight Update Norm\": aligned_wu_norms},\n",
    "                     filename=\"weight_update_norm_vs_epoch.png\", log_scale_y=True)\n",
    "\n",
    "    optimizer_state_history = raw_data.get('optimizer_state_history', [])\n",
    "    if optimizer_state_history:\n",
    "        aligned_m_norms = _align_epoch_metric(optimizer_state_history, epochs, 'avg_m_norm')\n",
    "        aligned_eff_lrs = _align_epoch_metric(optimizer_state_history, epochs, 'avg_eff_lr')\n",
    "        aligned_momentum_norms = _align_epoch_metric(optimizer_state_history, epochs, 'avg_momentum_norm')\n",
    "\n",
    "        adam_metrics_to_plot = {}\n",
    "        if aligned_m_norms and any(not np.isnan(x) for x in aligned_m_norms): adam_metrics_to_plot['Avg Adam m_norm'] = aligned_m_norms\n",
    "        if aligned_eff_lrs and any(not np.isnan(x) for x in aligned_eff_lrs): adam_metrics_to_plot['Avg Adam eff_LR'] = aligned_eff_lrs\n",
    "\n",
    "        sgd_metrics_to_plot = {}\n",
    "        if aligned_momentum_norms and any(not np.isnan(x) for x in aligned_momentum_norms): sgd_metrics_to_plot['Avg SGD Momentum Norm'] = aligned_momentum_norms\n",
    "\n",
    "        if adam_metrics_to_plot:\n",
    "            _plot_metric(6, \"Adam Optimizer State vs. Epoch\", \"Metric Value\",\n",
    "                         epoch_ticks=epoch_ticks, num_epochs=num_epochs, output_dir=output_plot_dir, flag_epochs_map=flag_epochs_map,\n",
    "                         other_metrics=adam_metrics_to_plot, filename=\"adam_state_vs_epoch.png\", log_scale_y=True)\n",
    "        if sgd_metrics_to_plot:\n",
    "            _plot_metric(7, \"SGD Optimizer State vs. Epoch\", \"Avg Momentum Norm\",\n",
    "                         epoch_ticks=epoch_ticks, num_epochs=num_epochs, output_dir=output_plot_dir, flag_epochs_map=flag_epochs_map,\n",
    "                         other_metrics=sgd_metrics_to_plot, filename=\"sgd_state_vs_epoch.png\", log_scale_y=True)\n",
    "\n",
    "    if aligned_noise_mags and any(not np.isnan(x) for x in aligned_noise_mags):\n",
    "        _plot_metric(8, \"Noise Magnitude vs. Epoch\", \"Magnitude\",\n",
    "                     epoch_ticks=epoch_ticks, num_epochs=num_epochs, output_dir=output_plot_dir, flag_epochs_map=flag_epochs_map,\n",
    "                     other_metrics={\"Noise Magnitude\": aligned_noise_mags},\n",
    "                     filename=\"noise_magnitude_vs_epoch.png\")\n",
    "\n",
    "\n",
    "    print(f\"Finished generating plots in {output_plot_dir}.\")\n"
   ],
   "id": "4e444c5b0b237eb0",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading metrics from: C:\\Users\\Home\\Documents\\EPFL\\OptML\\OptML\\PyTorch_CIFAR10\\checkpoints\\metrics\\baby_cnn_gradient_gaussian_0.01_20250609_230947\\all_metrics.json\n",
      "Saving plots to: my_experiment_plots\n",
      "Error: JSON file not found at C:\\Users\\Home\\Documents\\EPFL\\OptML\\OptML\\PyTorch_CIFAR10\\checkpoints\\metrics\\baby_cnn_gradient_gaussian_0.01_20250609_230947\\all_metrics.json\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-12T19:20:33.494770Z",
     "start_time": "2025-06-12T19:20:33.479715Z"
    }
   },
   "cell_type": "code",
   "source": [
    "json_file = r\"C:\\Users\\Home\\Documents\\EPFL\\OptML\\OptML\\PyTorch_CIFAR10\\checkpoints\\metrics\\baby_cnn_gradient_gaussian_0.01_20250609_193813\\all_metrics.json\" # <<< UPDATE THIS PATH\n",
    "output_plot_dir = \"my_experiment_plots\" # Folder to save plots\n",
    "\n",
    "# Create output directory if it doesn't exist\n",
    "os.makedirs(output_plot_dir, exist_ok=True)\n",
    "print(f\"Loading metrics from: {json_file}\")\n",
    "print(f\"Saving plots to: {output_plot_dir}\")\n",
    "\n",
    "# 2. Load and parse data\n",
    "if not os.path.exists(json_file):\n",
    "    print(f\"Error: JSON file not found at {json_file}\")\n",
    "else:\n",
    "    with open(json_file, 'r') as f:\n",
    "        raw_data = json.load(f)\n",
    "\n",
    "    epochs_data_dict = {}\n",
    "    flags_history_list = []\n",
    "\n",
    "    if isinstance(raw_data, dict):\n",
    "        print(f\"Info: Input file {json_file} is a dictionary. Parsing comprehensive metrics.\")\n",
    "        epochs_data_dict['epochs_completed'] = raw_data.get('epochs', [])\n",
    "        epochs_data_dict['train_loss_history'] = raw_data.get('train_loss_history', [])\n",
    "        epochs_data_dict['train_acc_history'] = raw_data.get('train_acc_history', [])\n",
    "        epochs_data_dict['val_loss_history'] = raw_data.get('val_loss_history', [])\n",
    "        epochs_data_dict['val_acc_history'] = raw_data.get('val_acc_history', [])\n",
    "        epochs_data_dict['lr_history'] = raw_data.get('lr_history', [])\n",
    "        flags_history_list = raw_data.get('optimization_flags_history', [])\n",
    "        # Correctly extract noise metrics\n",
    "        noise_metrics_history = raw_data.get('noise_metrics', [])\n",
    "\n",
    "    elif isinstance(raw_data, list): # Fallback for older JSON structure if needed\n",
    "        print(f\"Info: Input file {json_file} is a list. This format might not contain all desired metrics.\")\n",
    "        epochs_data_dict['epochs_completed'] = [r.get(\"epoch\") for r in raw_data if \"epoch\" in r]\n",
    "        epochs_data_dict['train_loss_history'] = [r.get(\"train_loss\") for r in raw_data if \"train_loss\" in r]\n",
    "        epochs_data_dict['train_acc_history'] = [r.get(\"train_acc\") for r in raw_data if \"train_acc\" in r]\n",
    "        epochs_data_dict['val_loss_history'] = [r.get(\"val_loss\") for r in raw_data if \"val_loss\" in r]\n",
    "        epochs_data_dict['val_acc_history'] = [r.get(\"val_acc\") for r in raw_data if \"val_acc\" in r]\n",
    "        epochs_data_dict['lr_history'] = [r.get(\"learning_rate\") for r in raw_data if \"learning_rate\" in r]\n",
    "        noise_metrics_history = [] # No noise metrics in this older format, or handle specifically\n",
    "        for key, val_list in epochs_data_dict.items():\n",
    "            epochs_data_dict[key] = [v for v in val_list if v is not None]\n",
    "    else:\n",
    "        print(f\"Error: Unknown JSON structure in {json_file}\")\n",
    "        # Exit or handle error appropriately if not running in a function\n",
    "        exit()\n",
    "\n",
    "    epochs = epochs_data_dict.get('epochs_completed', [])\n",
    "    if not epochs:\n",
    "        print(\"No epoch data found. Cannot generate plots.\")\n",
    "        exit() # Exit the script if no data\n",
    "\n",
    "    num_epochs = len(epochs)\n",
    "    epoch_ticks = np.array(epochs)\n",
    "\n",
    "    train_loss = epochs_data_dict.get('train_loss_history', [])\n",
    "    val_loss = epochs_data_dict.get('val_loss_history', [])\n",
    "    train_acc = [a * 100 if isinstance(a, (int, float)) else float('nan') for a in epochs_data_dict.get('train_acc_history', [])]\n",
    "    val_acc = [a * 100 if isinstance(a, (int, float)) else float('nan') for a in epochs_data_dict.get('val_acc_history', [])]\n",
    "    lr_history = epochs_data_dict.get('lr_history', [])\n",
    "\n",
    "\n",
    "    # Extract actual triggered flags\n",
    "    flag_epochs_map: Dict[int, List[str]] = {}\n",
    "    if flags_history_list:\n",
    "        for flag_event in flags_history_list:\n",
    "            epoch = flag_event.get('epoch')\n",
    "            flag_type = flag_event.get('flag_type', 'UNKNOWN_FLAG')\n",
    "            if epoch is not None:\n",
    "                if epoch not in flag_epochs_map:\n",
    "                    flag_epochs_map[epoch] = []\n",
    "                if flag_type not in flag_epochs_map[epoch]:\n",
    "                    flag_epochs_map[epoch].append(flag_type)\n",
    "    else:\n",
    "        print(\"No 'optimization_flags_history' found in JSON. Triggered flags from training will not be highlighted.\")\n",
    "\n",
    "    # Calculate metrics for plotting (even if not used for triggering)\n",
    "    epoch_avg_grad_norms = _aggregate_batch_metric_per_epoch(raw_data.get('gradient_norms', []), epochs, 'total_norm')\n",
    "    aligned_wu_norms = _align_epoch_metric(raw_data.get('weight_update_norm_history', []), epochs, 'norm')\n",
    "    aligned_noise_mags = _align_epoch_metric(noise_metrics_history, epochs, 'magnitude') # Corrected source for noise magnitudes\n",
    "\n",
    "    # 3. Generate Plots\n",
    "    _plot_metric(1, \"Training & Validation Loss vs. Epoch\", \"Loss\",\n",
    "                 epoch_ticks=epoch_ticks, num_epochs=num_epochs, output_dir=output_plot_dir, flag_epochs_map=flag_epochs_map,\n",
    "                 train_metric=train_loss, val_metric=val_loss,\n",
    "                 filename=\"loss_vs_epoch.png\", log_scale_y=True)\n",
    "\n",
    "    _plot_metric(2, \"Training & Validation Accuracy vs. Epoch\", \"Accuracy (%)\",\n",
    "                 epoch_ticks=epoch_ticks, num_epochs=num_epochs, output_dir=output_plot_dir, flag_epochs_map=flag_epochs_map,\n",
    "                 train_metric=train_acc, val_metric=val_acc,\n",
    "                 filename=\"accuracy_vs_epoch.png\")\n",
    "\n",
    "    _plot_metric(3, \"Learning Rate vs. Epoch\", \"Learning Rate\",\n",
    "                 epoch_ticks=epoch_ticks, num_epochs=num_epochs, output_dir=output_plot_dir, flag_epochs_map=flag_epochs_map,\n",
    "                 other_metrics={\"Learning Rate\": lr_history},\n",
    "                 filename=\"lr_vs_epoch.png\", log_scale_y=True)\n",
    "\n",
    "    if epoch_avg_grad_norms and any(not np.isnan(x) for x in epoch_avg_grad_norms):\n",
    "        _plot_metric(4, \"Average Gradient Norm vs. Epoch\", \"Avg L2 Norm of Gradients\",\n",
    "                     epoch_ticks=epoch_ticks, num_epochs=num_epochs, output_dir=output_plot_dir, flag_epochs_map=flag_epochs_map,\n",
    "                     other_metrics={\"Avg Grad Norm\": epoch_avg_grad_norms},\n",
    "                     filename=\"grad_norm_vs_epoch.png\", log_scale_y=True)\n",
    "\n",
    "    if aligned_wu_norms and any(not np.isnan(x) for x in aligned_wu_norms):\n",
    "        _plot_metric(5, \"Weight Update Norm vs. Epoch\", \"L2 Norm of Weight Updates\",\n",
    "                     epoch_ticks=epoch_ticks, num_epochs=num_epochs, output_dir=output_plot_dir, flag_epochs_map=flag_epochs_map,\n",
    "                     other_metrics={\"Weight Update Norm\": aligned_wu_norms},\n",
    "                     filename=\"weight_update_norm_vs_epoch.png\", log_scale_y=True)\n",
    "\n",
    "    optimizer_state_history = raw_data.get('optimizer_state_history', [])\n",
    "    if optimizer_state_history:\n",
    "        aligned_m_norms = _align_epoch_metric(optimizer_state_history, epochs, 'avg_m_norm')\n",
    "        aligned_eff_lrs = _align_epoch_metric(optimizer_state_history, epochs, 'avg_eff_lr')\n",
    "        aligned_momentum_norms = _align_epoch_metric(optimizer_state_history, epochs, 'avg_momentum_norm')\n",
    "\n",
    "        adam_metrics_to_plot = {}\n",
    "        if aligned_m_norms and any(not np.isnan(x) for x in aligned_m_norms): adam_metrics_to_plot['Avg Adam m_norm'] = aligned_m_norms\n",
    "        if aligned_eff_lrs and any(not np.isnan(x) for x in aligned_eff_lrs): adam_metrics_to_plot['Avg Adam eff_LR'] = aligned_eff_lrs\n",
    "\n",
    "        sgd_metrics_to_plot = {}\n",
    "        if aligned_momentum_norms and any(not np.isnan(x) for x in aligned_momentum_norms): sgd_metrics_to_plot['Avg SGD Momentum Norm'] = aligned_momentum_norms\n",
    "\n",
    "        if adam_metrics_to_plot:\n",
    "            _plot_metric(6, \"Adam Optimizer State vs. Epoch\", \"Metric Value\",\n",
    "                         epoch_ticks=epoch_ticks, num_epochs=num_epochs, output_dir=output_plot_dir, flag_epochs_map=flag_epochs_map,\n",
    "                         other_metrics=adam_metrics_to_plot, filename=\"adam_state_vs_epoch.png\", log_scale_y=True)\n",
    "        if sgd_metrics_to_plot:\n",
    "            _plot_metric(7, \"SGD Optimizer State vs. Epoch\", \"Avg Momentum Norm\",\n",
    "                         epoch_ticks=epoch_ticks, num_epochs=num_epochs, output_dir=output_plot_dir, flag_epochs_map=flag_epochs_map,\n",
    "                         other_metrics=sgd_metrics_to_plot, filename=\"sgd_state_vs_epoch.png\", log_scale_y=True)\n",
    "\n",
    "    if aligned_noise_mags and any(not np.isnan(x) for x in aligned_noise_mags):\n",
    "        _plot_metric(8, \"Noise Magnitude vs. Epoch\", \"Magnitude\",\n",
    "                     epoch_ticks=epoch_ticks, num_epochs=num_epochs, output_dir=output_plot_dir, flag_epochs_map=flag_epochs_map,\n",
    "                     other_metrics={\"Noise Magnitude\": aligned_noise_mags},\n",
    "                     filename=\"noise_magnitude_vs_epoch.png\")\n",
    "\n",
    "\n",
    "    print(f\"Finished generating plots in {output_plot_dir}.\")\n"
   ],
   "id": "26db27a421d137bb",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading metrics from: C:\\Users\\Home\\Documents\\EPFL\\OptML\\OptML\\PyTorch_CIFAR10\\checkpoints\\metrics\\baby_cnn_gradient_gaussian_0.01_20250609_193813\\all_metrics.json\n",
      "Saving plots to: my_experiment_plots\n",
      "Error: JSON file not found at C:\\Users\\Home\\Documents\\EPFL\\OptML\\OptML\\PyTorch_CIFAR10\\checkpoints\\metrics\\baby_cnn_gradient_gaussian_0.01_20250609_193813\\all_metrics.json\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-12T19:20:33.821932Z",
     "start_time": "2025-06-12T19:20:33.519026Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import json\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib.ticker import ScalarFormatter\n",
    "def plot_adaptive_noise_analysis(experiment_dir: str):\n",
    "    \"\"\"\n",
    "    Loads and plots experiment data to analyze adaptive noise triggers.\n",
    "    (Updated to work with the performant metrics.py)\n",
    "    \"\"\"\n",
    "    # --- 1. Define Correct File Paths ---\n",
    "    # FIX: The flag data is now in a .jsonl file.\n",
    "    flags_file = os.path.join(experiment_dir, 'optimization_flags.jsonl')\n",
    "    # FIX: All detailed metrics are now saved in this single file at the end of training.\n",
    "    all_metrics_file = os.path.join(experiment_dir, 'all_metrics.json')\n",
    "    epoch_metrics_file = os.path.join(experiment_dir, 'epoch_metrics.jsonl')\n",
    "\n",
    "    # --- 2. Load Data from New Sources ---\n",
    "    flags_df = None\n",
    "    grad_norm_data = {}\n",
    "    epoch_data = None\n",
    "\n",
    "    # Load flags data from .jsonl\n",
    "    try:\n",
    "        flags_df = pd.read_json(flags_file, lines=True)\n",
    "        print(f\"Successfully loaded {len(flags_df)} flag events from {flags_file}\")\n",
    "    except ValueError:\n",
    "        print(f\"Warning: Could not find or parse flags file at {flags_file}\")\n",
    "\n",
    "    # Load all metrics to get gradient norms\n",
    "    try:\n",
    "        with open(all_metrics_file, 'r') as f:\n",
    "            all_metrics = json.load(f)\n",
    "            # FIX: Gradient norms are now under the 'gradient_norms_by_epoch' key\n",
    "            grad_norm_data = all_metrics.get('gradient_norms_by_epoch', {})\n",
    "        print(f\"Successfully loaded gradient norm data for {len(grad_norm_data)} epochs from {all_metrics_file}\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Warning: Could not find all_metrics.json file at {all_metrics_file}\")\n",
    "    except (json.JSONDecodeError, AttributeError):\n",
    "        print(f\"Warning: Could not parse gradient norm data from {all_metrics_file}\")\n",
    "\n",
    "    # Load per-epoch data from the JSON Lines file\n",
    "    try:\n",
    "        epoch_data = pd.read_json(epoch_metrics_file, lines=True)\n",
    "        print(f\"Successfully loaded epoch data for {len(epoch_data)} epochs.\")\n",
    "    except ValueError:\n",
    "        print(f\"Warning: Could not find or parse epoch metrics file at {epoch_metrics_file}\")\n",
    "\n",
    "    # --- 3. Process Data ---\n",
    "    # FIX: Process the new dictionary structure for gradient norms\n",
    "    avg_grad_norms = pd.DataFrame()\n",
    "    if grad_norm_data:\n",
    "        # Flatten the dictionary of lists into a single list of records\n",
    "        flat_grad_list = [item for epoch_list in grad_norm_data.values() for item in epoch_list]\n",
    "        if flat_grad_list:\n",
    "            grad_df = pd.DataFrame(flat_grad_list)\n",
    "            avg_grad_norms = grad_df.groupby('epoch')['total_norm'].mean().reset_index()\n",
    "\n",
    "    # --- 4. Create the Plot (Plotting logic remains mostly the same) ---\n",
    "    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(16, 12), sharex=True,\n",
    "                                   gridspec_kw={'height_ratios': [3, 1]})\n",
    "    fig.suptitle(f'Adaptive Noise Analysis\\n({os.path.basename(experiment_dir)})', fontsize=16)\n",
    "\n",
    "    # Top Subplot: Gradient Norms and Validation Loss\n",
    "    ax1.set_ylabel('Avg. Gradient Norm (Log Scale)', color='tab:blue')\n",
    "    ax1.set_yscale('log')\n",
    "    if not avg_grad_norms.empty:\n",
    "        ax1.plot(avg_grad_norms['epoch'], avg_grad_norms['total_norm'], 'o-', label='Avg. Gradient Norm', color='tab:blue', alpha=0.8)\n",
    "    ax1.tick_params(axis='y', labelcolor='tab:blue')\n",
    "    ax1.grid(True, which='both', linestyle='--', linewidth=0.5)\n",
    "    ax1.yaxis.set_major_formatter(ScalarFormatter())\n",
    "    ax1.set_ylim(0.0,20.0)\n",
    "    ax1_twin = ax1.twinx()\n",
    "    ax1_twin.set_ylabel('Validation Loss', color='tab:red')\n",
    "    if epoch_data is not None and 'val_loss' in epoch_data.columns:\n",
    "        ax1_twin.plot(epoch_data['epoch'], epoch_data['val_loss'], 'o-', label='Validation Loss', color='tab:red', alpha=0.8)\n",
    "    ax1_twin.tick_params(axis='y', labelcolor='tab:red')\n",
    "\n",
    "    # Bottom Subplot: Flag Triggers\n",
    "    ax2.set_ylabel('Flag Events')\n",
    "    ax2.set_xlabel('Epoch')\n",
    "    ax2.set_yticks([])\n",
    "    ax2.grid(True, linestyle='--', linewidth=0.5)\n",
    "\n",
    "    flag_colors = {\"GRAD_NORM_PLATEAU\": \"cyan\", \"LOW_WEIGHT_UPDATE_NORM\": \"gold\", \"VAL_LOSS_PLATEAU\": \"red\", \"OVERFITTING\": \"purple\"}\n",
    "\n",
    "    plotted_labels = set()\n",
    "    if flags_df is not None:\n",
    "        for _, flag in flags_df.iterrows():\n",
    "            epoch = flag['epoch']\n",
    "            flag_type = flag['flag_type']\n",
    "            color = flag_colors.get(flag_type, 'gray')\n",
    "            label = f'Flag: {flag_type}' if flag_type not in plotted_labels else None\n",
    "            ax2.axvline(x=epoch, color=color, linestyle='--', linewidth=2, label=label)\n",
    "            ax1.axvline(x=epoch, color=color, linestyle=':', linewidth=1.5, alpha=0.7)\n",
    "            plotted_labels.add(flag_type)\n",
    "\n",
    "    if epoch_data is not None and not epoch_data.empty:\n",
    "        ax2.set_xlim(0, epoch_data['epoch'].max() + 1)\n",
    "\n",
    "    lines1, labels1 = ax1.get_legend_handles_labels()\n",
    "    lines1_twin, labels1_twin = ax1_twin.get_legend_handles_labels()\n",
    "    ax1.legend(lines1 + lines1_twin, labels1 + labels1_twin, loc='upper left')\n",
    "\n",
    "    if len(plotted_labels) > 0:\n",
    "        ax2.legend(loc='best')\n",
    "\n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# --- USAGE EXAMPLE ---\n",
    "# Use the same experiment path as before\n",
    "experiment_path = r\"C:\\Users\\Home\\Documents\\EPFL\\OptML\\OptML\\PyTorch_CIFAR10\\checkpoints\\metrics\\baby_cnn_none_20250612_133059\"\n",
    "\n",
    "if os.path.isdir(experiment_path):\n",
    "    plot_adaptive_noise_analysis(experiment_path)\n",
    "else:\n",
    "    print(f\"Directory not found: '{experiment_path}'\")\n",
    "    print(\"Please update the 'experiment_path' variable with the correct path to your experiment's metrics folder.\")"
   ],
   "id": "83f2f86a40458687",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory not found: 'C:\\Users\\Home\\Documents\\EPFL\\OptML\\OptML\\PyTorch_CIFAR10\\checkpoints\\metrics\\baby_cnn_none_20250612_133059'\n",
      "Please update the 'experiment_path' variable with the correct path to your experiment's metrics folder.\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-12T19:20:33.835674Z",
     "start_time": "2025-06-12T19:20:33.832732Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# --- USAGE EXAMPLE ---\n",
    "# Use the same experiment path as before\n",
    "experiment_path = r\"C:\\Users\\Home\\Documents\\EPFL\\OptML\\OptML\\PyTorch_CIFAR10\\checkpoints\\metrics\\resnet18_gradient_gaussian_0.01_20250611_100440\"\n",
    "\n",
    "if os.path.isdir(experiment_path):\n",
    "    plot_adaptive_noise_analysis(experiment_path)\n",
    "else:\n",
    "    print(f\"Directory not found: '{experiment_path}'\")\n",
    "    print(\"Please update the 'experiment_path' variable with the correct path to your experiment's metrics folder.\")"
   ],
   "id": "49515e8791ae397a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory not found: 'C:\\Users\\Home\\Documents\\EPFL\\OptML\\OptML\\PyTorch_CIFAR10\\checkpoints\\metrics\\resnet18_gradient_gaussian_0.01_20250611_100440'\n",
      "Please update the 'experiment_path' variable with the correct path to your experiment's metrics folder.\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-12T19:20:33.860098Z",
     "start_time": "2025-06-12T19:20:33.855891Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# --- USAGE EXAMPLE ---\n",
    "# Use the same experiment path as before\n",
    "experiment_path = r\"C:\\Users\\Home\\Documents\\EPFL\\OptML\\OptML\\PyTorch_CIFAR10\\checkpoints\\metrics\\baby_cnn_none_20250611_170125\"\n",
    "\n",
    "if os.path.isdir(experiment_path):\n",
    "    plot_adaptive_noise_analysis(experiment_path)\n",
    "else:\n",
    "    print(f\"Directory not found: '{experiment_path}'\")\n",
    "    print(\"Please update the 'experiment_path' variable with the correct path to your experiment's metrics folder.\")"
   ],
   "id": "2a356704d47e19ea",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory not found: 'C:\\Users\\Home\\Documents\\EPFL\\OptML\\OptML\\PyTorch_CIFAR10\\checkpoints\\metrics\\baby_cnn_none_20250611_170125'\n",
      "Please update the 'experiment_path' variable with the correct path to your experiment's metrics folder.\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-12T19:20:33.888794Z",
     "start_time": "2025-06-12T19:20:33.886071Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# --- USAGE EXAMPLE ---\n",
    "# Use the same experiment path as before\n",
    "experiment_path = r\"C:\\Users\\Home\\Documents\\EPFL\\OptML\\OptML\\PyTorch_CIFAR10\\checkpoints\\metrics\\baby_cnn_none_20250611_151504\"\n",
    "\n",
    "if os.path.isdir(experiment_path):\n",
    "    plot_adaptive_noise_analysis(experiment_path)\n",
    "else:\n",
    "    print(f\"Directory not found: '{experiment_path}'\")\n",
    "    print(\"Please update the 'experiment_path' variable with the correct path to your experiment's metrics folder.\")"
   ],
   "id": "7fb338f6761da129",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory not found: 'C:\\Users\\Home\\Documents\\EPFL\\OptML\\OptML\\PyTorch_CIFAR10\\checkpoints\\metrics\\baby_cnn_none_20250611_151504'\n",
      "Please update the 'experiment_path' variable with the correct path to your experiment's metrics folder.\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-12T19:20:33.926725Z",
     "start_time": "2025-06-12T19:20:33.914638Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import json\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "#import seaborn as sns # Using seaborn for better aesthetics\n",
    "def load_flag_data(experiment_dir: str):\n",
    "    \"\"\"Loads the optimization flag data from a .jsonl file.\"\"\"\n",
    "    flags_file = os.path.join(experiment_dir, 'optimization_flags.jsonl')\n",
    "    try:\n",
    "        flags_df = pd.read_json(flags_file, lines=True)\n",
    "        print(f\"Successfully loaded {len(flags_df)} flag events.\")\n",
    "        return flags_df\n",
    "    except (ValueError, FileNotFoundError):\n",
    "        print(\" No optimization flags file found or it's empty.\")\n",
    "        return None\n",
    "\n",
    "def plot_flag_events(ax, flags_df):\n",
    "    \"\"\"Plots vertical lines on a given axis to indicate when flags were triggered.\"\"\"\n",
    "    if flags_df is None or flags_df.empty:\n",
    "        return\n",
    "\n",
    "    flag_colors = {\n",
    "        \"GRAD_NORM_PLATEAU\": \"cyan\",\n",
    "        \"LOW_WEIGHT_UPDATE_NORM\": \"gold\",\n",
    "        \"VAL_LOSS_PLATEAU\": \"magenta\",\n",
    "        \"OVERFITTING\": \"red\"\n",
    "    }\n",
    "\n",
    "    # Use a small y-offset for each flag type to avoid overlap if they occur on the same epoch\n",
    "    y_offsets = {flag: i * 0.1 for i, flag in enumerate(flag_colors.keys())}\n",
    "\n",
    "    plotted_labels = set()\n",
    "    for _, flag in flags_df.iterrows():\n",
    "        epoch = flag['epoch']\n",
    "        flag_type = flag['flag_type']\n",
    "        color = flag_colors.get(flag_type, 'gray')\n",
    "        label = f'Flag: {flag_type}' if flag_type not in plotted_labels else None\n",
    "\n",
    "        # Plot a vertical line on the main axis\n",
    "        ax.axvline(x=epoch, color=color, linestyle=':', linewidth=1.5, alpha=0.7, label=label)\n",
    "        plotted_labels.add(flag_type)\n",
    "\n",
    "def plot_overall_training_evolution(experiment_dir: str):\n",
    "    \"\"\"\n",
    "    Loads and plots the overall training evolution metrics, including flag events.\n",
    "    \"\"\"\n",
    "    print(f\" Analyzing Overall Metrics for: {os.path.basename(experiment_dir)}\\n\")\n",
    "\n",
    "    # --- 1. Load Data ---\n",
    "    epoch_metrics_file = os.path.join(experiment_dir, 'epoch_metrics.jsonl')\n",
    "    noise_file = os.path.join(experiment_dir, 'noise_metrics.jsonl')\n",
    "\n",
    "    try:\n",
    "        epoch_df = pd.read_json(epoch_metrics_file, lines=True)\n",
    "        print(f\"Successfully loaded {len(epoch_df)} epochs from {epoch_metrics_file}\")\n",
    "    except (ValueError, FileNotFoundError):\n",
    "        print(f\" Error: Could not find or parse epoch metrics file at {epoch_metrics_file}\")\n",
    "        return\n",
    "\n",
    "    noise_df = None\n",
    "    try:\n",
    "        noise_df = pd.read_json(noise_file, lines=True).drop_duplicates(subset='epoch', keep='first')\n",
    "        print(f\"Successfully loaded {len(noise_df)} noise events from {noise_file}\")\n",
    "    except (ValueError, FileNotFoundError):\n",
    "        print(\" No noise metrics file found, skipping noise plot.\")\n",
    "\n",
    "    # Load flag data using the helper function\n",
    "    flags_df = load_flag_data(experiment_dir)\n",
    "\n",
    "    # --- 2. Create Plots ---\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(20, 12), sharex=True)\n",
    "    fig.suptitle(f'Overall Training Evolution & Noise Events\\n({os.path.basename(experiment_dir)})', fontsize=18)\n",
    "\n",
    "    # Plot on all axes\n",
    "    for ax in axes.flatten():\n",
    "        plot_flag_events(ax, flags_df)\n",
    "\n",
    "    # Plot 1: Loss\n",
    "    ax1 = axes[0, 0]\n",
    "    ax1.plot(epoch_df['epoch'], epoch_df['train_loss'], 'o-', label='Training Loss', color='tab:blue', markersize=4)\n",
    "    ax1.plot(epoch_df['epoch'], epoch_df['val_loss'], 'o-', label='Validation Loss', color='tab:orange', markersize=4)\n",
    "    ax1.set_title('Training vs. Validation Loss')\n",
    "    ax1.set_ylabel('Loss')\n",
    "    ax1.grid(True, linestyle='--', alpha=0.6)\n",
    "\n",
    "    # Plot 2: Accuracy\n",
    "    ax2 = axes[0, 1]\n",
    "    ax2.plot(epoch_df['epoch'], epoch_df['train_acc'], 'o-', label='Training Accuracy', color='tab:blue', markersize=4)\n",
    "    ax2.plot(epoch_df['epoch'], epoch_df['val_acc'], 'o-', label='Validation Accuracy', color='tab:orange', markersize=4)\n",
    "    ax2.yaxis.set_major_formatter(plt.FuncFormatter(lambda y, _: f'{y:.0%}'))\n",
    "    ax2.set_title('Training vs. Validation Accuracy')\n",
    "    ax2.set_ylabel('Accuracy')\n",
    "    ax2.grid(True, linestyle='--', alpha=0.6)\n",
    "\n",
    "    # Plot 3: Learning Rate\n",
    "    ax3 = axes[1, 0]\n",
    "    ax3.plot(epoch_df['epoch'], epoch_df['learning_rate'], 'o-', label='Learning Rate', color='tab:green', markersize=4)\n",
    "    ax3.set_title('Learning Rate Schedule')\n",
    "    ax3.set_xlabel('Epoch')\n",
    "    ax3.set_ylabel('Learning Rate')\n",
    "    ax3.set_yscale('log')\n",
    "    ax3.grid(True, which=\"both\", linestyle='--', alpha=0.6)\n",
    "\n",
    "    # Plot 4: Noise Magnitude\n",
    "    ax4 = axes[1, 1]\n",
    "    if noise_df is not None and not noise_df.empty:\n",
    "        ax4.plot(noise_df['epoch'], noise_df['magnitude'], 'o-', label='Noise Magnitude', color='tab:purple', markersize=4)\n",
    "    ax4.set_title('Noise Magnitude Schedule')\n",
    "    ax4.set_xlabel('Epoch')\n",
    "    ax4.set_ylabel('Magnitude')\n",
    "    ax4.grid(True, linestyle='--', alpha=0.6)\n",
    "\n",
    "    # Create a single unified legend for the figure\n",
    "    handles, labels = [], []\n",
    "    for ax in fig.axes:\n",
    "        for h, l in zip(*ax.get_legend_handles_labels()):\n",
    "            if l not in labels:\n",
    "                handles.append(h)\n",
    "                labels.append(l)\n",
    "    fig.legend(handles, labels, loc='upper right', bbox_to_anchor=(1.0, 0.95))\n",
    "\n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
    "    plt.show()"
   ],
   "id": "a25d5ed5e6c1d005",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-12T19:20:33.954094Z",
     "start_time": "2025-06-12T19:20:33.946750Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def plot_class_metrics(experiment_dir: str, top_n: int = 5):\n",
    "    \"\"\"\n",
    "    Loads and plots per-class metrics, including flag events.\n",
    "    \"\"\"\n",
    "    print(f\" Analyzing Per-Class Metrics for: {os.path.basename(experiment_dir)}\\n\")\n",
    "\n",
    "    # --- 1. Load Data ---\n",
    "    epoch_metrics_file = os.path.join(experiment_dir, 'epoch_metrics.jsonl')\n",
    "    try:\n",
    "        epoch_df = pd.read_json(epoch_metrics_file, lines=True)\n",
    "    except (ValueError, FileNotFoundError):\n",
    "        print(f\" Error: Could not find or parse epoch metrics file at {epoch_metrics_file}\")\n",
    "        return\n",
    "\n",
    "    flags_df = load_flag_data(experiment_dir)\n",
    "\n",
    "    # --- 2. Unpack Nested JSON data ---\n",
    "    def unpack_metrics(df, column_name):\n",
    "        if column_name not in df.columns or df[column_name].isnull().all():\n",
    "            return None\n",
    "        records = []\n",
    "        for _, row in df.dropna(subset=[column_name]).iterrows():\n",
    "            for class_name, metrics in row[column_name].items():\n",
    "                records.append({'epoch': row['epoch'], 'class_name': class_name, **metrics})\n",
    "        return pd.DataFrame(records)\n",
    "\n",
    "    val_class_df = unpack_metrics(epoch_df, 'validation_per_class_metrics')\n",
    "\n",
    "    if val_class_df is None:\n",
    "        print(\" No detailed validation per-class metrics found to plot.\")\n",
    "        return\n",
    "\n",
    "    # --- 3. Find Best and Worst Classes ---\n",
    "    final_epoch_df = val_class_df[val_class_df['epoch'] == val_class_df['epoch'].max()].sort_values(by='accuracy')\n",
    "    worst_classes = final_epoch_df.head(top_n)['class_name'].tolist()\n",
    "    best_classes = final_epoch_df.tail(top_n)['class_name'].tolist()\n",
    "\n",
    "    # --- 4. Create Plots ---\n",
    "    fig, ax1 = plt.subplots(1, 1, figsize=(16, 8)) # Simplified to one plot for clarity\n",
    "    fig.suptitle(f'Per-Class Validation Accuracy & Noise Events\\n({os.path.basename(experiment_dir)})', fontsize=18)\n",
    "\n",
    "    # Plot flag events on the main axis\n",
    "    plot_flag_events(ax1, flags_df)\n",
    "\n",
    "    cmap = plt.get_cmap('coolwarm')\n",
    "    palette = [cmap(i) for i in np.linspace(0, 1, top_n * 2)]\n",
    "\n",
    "    for i, class_name in enumerate(worst_classes):\n",
    "        subset_df = val_class_df[val_class_df['class_name'] == class_name]\n",
    "        ax1.plot(subset_df['epoch'], subset_df['accuracy'], label=f\"Worst: {class_name}\", color=palette[i])\n",
    "\n",
    "    for i, class_name in enumerate(best_classes):\n",
    "        subset_df = val_class_df[val_class_df['class_name'] == class_name]\n",
    "        ax1.plot(subset_df['epoch'], subset_df['accuracy'], label=f\"Best: {class_name}\", color=palette[top_n + i], linestyle='--')\n",
    "\n",
    "    ax1.set_title(f'Top & Bottom {top_n} Class Accuracy Evolution')\n",
    "    ax1.set_xlabel('Epoch')\n",
    "    ax1.set_ylabel('Accuracy')\n",
    "    ax1.yaxis.set_major_formatter(plt.FuncFormatter(lambda y, _: f'{y:.0%}'))\n",
    "\n",
    "    # Create a single unified legend\n",
    "    handles, labels = ax1.get_legend_handles_labels()\n",
    "    fig.legend(handles, labels, loc='upper right', bbox_to_anchor=(1.15, 0.9))\n",
    "\n",
    "    ax1.grid(True, linestyle='--', alpha=0.6)\n",
    "\n",
    "    plt.tight_layout(rect=[0, 0, 0.85, 0.95])\n",
    "    plt.show()"
   ],
   "id": "6d507ea5c44175ac",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-12T19:20:33.970062Z",
     "start_time": "2025-06-12T19:20:33.967341Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# --- USAGE EXAMPLE ---\n",
    "\n",
    "# Make sure this path points to the specific experiment folder you want to analyze\n",
    "experiment_path = r\"C:\\Users\\Home\\Documents\\EPFL\\OptML\\OptML\\PyTorch_CIFAR10\\checkpoints\\metrics\\baby_cnn_none_20250612_133059\"\n",
    "\n",
    "if os.path.isdir(experiment_path):\n",
    "    # Plot the overall loss, accuracy, LR, and noise\n",
    "    plot_overall_training_evolution(experiment_path)\n",
    "\n",
    "    # Plot the detailed per-class metrics\n",
    "    plot_class_metrics(experiment_path, top_n=5)\n",
    "else:\n",
    "    print(f\"Directory not found: '{experiment_path}'\")"
   ],
   "id": "29e2bf6161550719",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory not found: 'C:\\Users\\Home\\Documents\\EPFL\\OptML\\OptML\\PyTorch_CIFAR10\\checkpoints\\metrics\\baby_cnn_none_20250612_133059'\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-12T19:20:34.256710Z",
     "start_time": "2025-06-12T19:20:34.254309Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "f3430e864aa5a85a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-12T19:20:34.393051Z",
     "start_time": "2025-06-12T19:20:34.391326Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "265121fd54fcd3e4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-12T19:20:35.116731Z",
     "start_time": "2025-06-12T19:20:35.114755Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "e47b039666ecbf34",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-12T19:20:35.298442Z",
     "start_time": "2025-06-12T19:20:35.296767Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "72b039db8b0735b5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "e489556b5b35c27c"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
